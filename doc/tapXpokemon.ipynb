{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "suffering-louisville",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# TAP x Pokemon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternate-overall",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "swedish-conversation",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "TAP x Pokemon is a project that links together many technologies and applies them to the world of competitive Pokemon battling, processing battle data logs to extract useful information and trying to predict the outcome of a match using machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-dominican",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"./img/pikachu_meme.png\" alt=\"Best Pikachu Meme\" title=\"Wow\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-criterion",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Project Introduction\n",
    "### Technicalities First"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "terminal-aluminum",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"./img/data_pipeline.png\" title=\"Data Pipeline\" width=\"1000\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integrated-exhibition",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "During this journey we are going to explore every data pipeline stage and its component, from *Ingestion* to *Visualization*.\n",
    "But first..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "willing-pilot",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Source\n",
    "## Where everything starts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-facing",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Pokemon Showdown\n",
    "## [Showdown Battle Simulator](https://play.pokemonshowdown.com/)\n",
    "\n",
    "\n",
    "\n",
    "As mentioned we are going to collect data regarding competitive pokemon battles, so we chose the best and most played competive simulator: Pokemon Showdown (aka SD).\n",
    "The website doesn't have any API to use and the only way to obtain data is to deal with the messages passing through the websocket, briefly...a nightmare!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-professor",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thankfully a hero made an excellent job, not just with host to server communication but creating a battle-bot using 3 different AIs (based on their behavior), his name is pmariglia and his work is available at [pmariglia/showdown](https://github.com/pmariglia/showdown).\n",
    "All I had to do was fixing the output, retrieving only the useful fields for the project and creating a json for every battle. Eventually everything is sent to our Data Ingestion component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-private",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " As an example:\n",
    " ***\n",
    " `{\n",
    "  \"battle\": [\n",
    "    {\n",
    "      \"bid\": 3,\n",
    "      \"player\": \"Norne42\",\n",
    "      \"type\": \"gen8ou\",\n",
    "      \"turns\": 44\n",
    "    }\n",
    "  ],\n",
    "  \"pokemon\": [\n",
    "    {\n",
    "      \"name\": \"barraskewda\",\n",
    "      \"ability\": \"swiftswim\",\n",
    "      \"types\": [\n",
    "        \"water\"\n",
    "      ],\n",
    "      \"item\": \"choiceband\",\n",
    "      \"win\": 1\n",
    "    },\n",
    "}` \n",
    "***\n",
    "Shortened for obvious reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tender-terrace",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In detail we are extracting data such as:\n",
    " - Battle Info (Player name, Turns, Type of Battle)\n",
    " - Pokemon Info (Name, Ability, Types, Moves, etc...)\n",
    " \n",
    "<img src=\"./img/more_data.jpg\" title=\"More Data\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-institute",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Ingestion\n",
    "## Where everything goes\n",
    "\n",
    "Logstash is our choice for this project and there many reasons why:\n",
    " - Ease of use, in both coding and \"Dockerizing\"\n",
    " - Filters and parse data on the fly\n",
    " - Flume didn't work...\n",
    " \n",
    " <img src=\"./img/flume_logstash.jpg\" title=\"Flume vs Logstash\" width=\"300\" />\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-environment",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " It just takes a bunch of logstash plugins (available in a unified elastic github for input and output), a python-logstash module and a few lines of code to simply send json data from the battle bots to Logstash. \n",
    "  <img src=\"./img/python_logstash.png\" title=\"Flume vs Logstash\" width=\"500\" align=\"center\"/>\n",
    "  Then from Logstash to Kafka Server to create a \"showdown\" topic.\n",
    " <img src=\"./img/logstash_code.png\" title=\"Flume vs Logstash\" width=\"350\" align=\"center\" />\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-shopping",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Maybe not that simple...\n",
    " <img src=\"./img/conf_meme.png\" title=\"Flume vs Logstash\" width=\"350\" align=\"center\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranging-stone",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Streaming\n",
    "## Apache Zookeeper & Kafka\n",
    "\n",
    "\n",
    "<img src=\"./img/zk_meme.png\" title=\"ZkLogoMeme\" width=\"350\" align=\"right\" align=\"center\" />\n",
    "\n",
    "\n",
    "**Zookeeper** is a top-level software developed by Yahoo first and Apache later, that acts as a centralized service and is used to maintain naming and configuration data and to provide flexible and robust synchronization within distributed systems.\n",
    "\n",
    "\n",
    "It keeps track of status of the Kafka cluster nodes and it also keeps track of Kafka topics, partitions etc. Zookeeper itself is allowing multiple clients to perform simultaneous reads and writes and acts as a shared configuration service within the system.\n",
    "\n",
    "\n",
    "Briefly, ZK is the service needed to keep Kafka online, nearly deprecated nowadays because of the new enterprise solution, Confluent, leaving the open-source formula for an open Core one.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### [Source1](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwiQndG43NzuAhVF-6QKHb6BCtIQFjAAegQIBBAD&url=https%3A%2F%2Fzookeeper.apache.org%2F&usg=AOvVaw0JEPBmJTTzUiLMgvke9BFd) [Source2](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwjJzdmd29zuAhUKO-wKHRf3CssQFjAEegQICRAC&url=https%3A%2F%2Fwww.cloudkarafka.com%2Fblog%2F2018-07-04-cloudkarafka_what_is_zookeeper.html&usg=AOvVaw1VL7xh_6qQl8Fsql0trjv2)  [Source3](https://www.confluent.io/confluent-community-license-faq/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-syracuse",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In a few words **Kafka** is distributed event streaming platform used for high-performance data pipelines, streaming analytics, data integration, and mission-critical applications (well maybe not mission-critical in this case). Here its usage is limited to a data pipeline and streaming service to pass events from Logstash to Spark in an easy and reliable way. Three are the key capabilities that can be implemented for event streaming:\n",
    "1) To *publish* (write) and *subscribe* to (read) streams of events, including continuous import/export of your data from other systems. This can be done using a Producer and Consumer written in any supported programming language or with a **Direct Approach** as in this case to bypass any overhead and avoid an addictional point of failure.\n",
    "\n",
    "2)  To *store* streams of events durably and reliably for as long as you want.\n",
    "\n",
    "3)  To *process* streams of events as they occur or retrospectively, an added value to this already versitile platform.\n",
    "\n",
    "#### [Source1](https://kafka.apache.org) [Source2](https://kafka.apache.org/documentation/#gettingStarted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-threat",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Processing\n",
    "## Let's get Started!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-methodology",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Apache Spark\n",
    "\n",
    "**Spark** is where everything takes shape, going from a simple json formatted event to a dataframe used to train a ML model, trying to predict the battle outcome based on the opponent team on the fly. Let's go in order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sporting-frost",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Spark Dataframe (dataframe.py)\n",
    "Implemented using the *pyspark.sql* module specifically designed for structured data processing, it is fondamentally used to create multiple dataframes all over this project (each of them stained with a little of my unexperienced python programmer blood).\n",
    "\n",
    "The **first** use case (dataframe.py) presents a simple dataframe that consists of two columns, one for the opponent team and the second for the battle score.\n",
    "The **second** and more complex one (showdown_es.py) is filled with all the useful data collected and processed along the pipeline. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secret-dictionary",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Spark MLlib (training.py)\n",
    "Advertized as easy to use...surprisingly it is! In this instance we built a Pipeline wich consists in 3 stages.\n",
    "\n",
    " - **RegexTokenizer**: taking a string as an input and returns as much tokens as the number of Pokemon in the input string.\n",
    " - **Word2Vec**: creates a word embedding for each token. Word2vec takes as its input a corpus of text and produces a vector space, with a variable dimension, with each unique word in the corpus being assigned a corresponding vector in the space. Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located close to one another in the space.\n",
    " - **Logistic Regression**: used as form of binary regression to predict one of the two possibile labels \"win/lose\" (1/0) for the word embeddings in input.\n",
    "\n",
    "The dataframe generated from the previous step is then split into two for model training, than it is evaluated and \"optimized\" and eventually saved to later use without the training overhead.\n",
    " \n",
    " #### [Source1](https://en.wikipedia.org/wiki/Word2vec) [Source2](https://en.wikipedia.org/wiki/Logistic_regression) [Example](https://blog.insightdatascience.com/hero2vec-d42d6838c941)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "union-works",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Spark x Pokemon  (showdown_es.py)\n",
    "The final step is where everything gets combined, the model trained on the original dataframe is used to make actual prediction on live data coming from the battling bots, expanding the incoming RDD and sending everything to Elasticsearch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-farming",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ElasticSearch & Kibana\n",
    "## The worst is over...or maybe not\n",
    "\n",
    "Both part of Elastic Stack, are essential to give our data expressive power but not without some trouble!\n",
    "\n",
    "\n",
    "**ElasticSearch** is used for storing and indexing data, providing an extremely fast search engine. All we have to do is sending a very simple and intuitive mapping...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-barrier",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " <img src=\"./img/mapping.png\" title=\"Mapping\" width=\"1000\" align=\"center\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-remains",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "...to create an incredibly flexible index...\n",
    " <img src=\"./img/MemeAttack.webp\" title=\"Mapping meme\" width=\"800\" align=\"top\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-organic",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Last but not least\n",
    "**Kibana** is the only component that doesn't need a thousand words to describe. A very powerful UI to visualize and navigate our data.\n",
    "\n",
    "**A picture's worth a thousand log lines**\n",
    "\n",
    "[source](https://www.elastic.co/kibana)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-conditioning",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " <img src=\"./img/dashboard.png\" title=\"Kibana Dashboard\" width=\"1500\" align=\"center\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-impression",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusions\n",
    "## That's it!\n",
    "\n",
    "This is the end of our journey, the whole project has been a great approach to everyday technologies composing a typical data pipeline. During this month I had the chance to learn a lot more than I expected in the first place, so I feel to thank my professor Salvatore Nicotra and a lot of pleople I don't even know sharing (and solving!) the problems I encountered. \n",
    "\n",
    " <img src=\"./img/thanks.jpg\" title=\"Thanks\" width=\"500\" align=\"center\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-official",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Future Improvements\n",
    "\n",
    "Sadly this project is far from perfect but luckily there is room for a lot of improvementes:\n",
    "\n",
    " - More variables taken in account to predict a win or a lose and therefore improve accuracy.\n",
    " - More data coming from the source about full pokemon sets and not just what a bot sees in a battle.\n",
    " - Live prediction during battle.\n",
    " - Reccomendation system for a competitive team.\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
